{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM2WSwoExMV8QDRc6qp6D+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KILjungjoon/Python_Grammar/blob/main/%EC%97%AC%EB%9F%AC_%EB%AC%B8%EC%84%9C%EB%A5%BC_%ED%95%9C_%EA%B0%9C%EC%9D%98_list_%EC%95%88%EC%97%90_token_list%EB%A1%9C_%EB%84%A3%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB1p1w4JEYCT",
        "outputId": "a5c9291d-2a75-4281-a74f-8155fcc22ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic'], ['health', 'expert', 'suggest', 'drive', 'may', 'caus', 'increas', 'tension', 'blood', 'pressur'], ['often', 'feel', 'pressur', 'perform', 'well', 'school', 'mother', 'never', 'seem', 'drive', 'brother', 'better'], ['health', 'profession', 'say', 'brocolli', 'good', 'health'], ['big', 'data', 'term', 'use', 'refer', 'data', 'set', 'larg', 'complex', 'tradit', 'data', 'process', 'applic', 'softwar', 'adequ', 'deal'], ['data', 'mani', 'case', 'offer', 'greater', 'statist', 'power', 'data', 'higher', 'complex', 'may', 'lead', 'higher', 'fals', 'discoveri', 'rate'], ['big', 'data', 'origin', 'associ', 'three', 'key', 'concept', 'volum', 'varieti', 'veloc'], ['2016', 'definit', 'state', 'big', 'data', 'repres', 'inform', 'asset', 'character', 'high', 'volum', 'veloc', 'varieti', 'requir', 'specif', 'technolog', 'analyt', 'method', 'transform', 'valu'], ['data', 'must', 'process', 'advanc', 'tool', 'reveal', 'meaning', 'inform']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "tokenizer = RegexpTokenizer('[\\w]+')\n",
        "stop_words = stopwords.words('english')\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
        "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
        "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
        "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
        "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
        "doc_f = \"Big data is a term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.\"\n",
        "doc_g = \"Data with many cases offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate\"\n",
        "doc_h = \"Big data was originally associated with three key concepts: volume, variety, and velocity.\"\n",
        "doc_i = \"A 2016 definition states that 'Big data represents the information assets characterized by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value'.\"\n",
        "doc_j = \"Data must be processed with advanced tools to reveal meaningful information.\"\n",
        "\n",
        "\n",
        "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]\n",
        "\n",
        "texts = []\n",
        "for w in doc_set:\n",
        "    raw = w.lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
        "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "    texts.append(stemmed_tokens)\n",
        "\n",
        "print(texts)"
      ]
    }
  ]
}